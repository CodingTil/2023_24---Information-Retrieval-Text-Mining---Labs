{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmP-eKJBoH5C"
      },
      "source": [
        "# Experiments with text classifiers in sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUwXFpsboH5E"
      },
      "source": [
        "In this exercise we'll be experimenting with various classification algorithms in scikit learn using the [20 Newsgroups collection](http://people.csail.mit.edu/jrennie/20Newsgroups/).\n",
        "\n",
        "The first part of the notebook shows a detailed example usage of text classification using sklearn (based on [scikit learn's \"Working with text data\" tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)).\n",
        "The real exercise is at the bottom, where you'll be asked to perform various experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP_joU_yoH5F"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXjg-tcToH5F"
      },
      "source": [
        "In order to get faster execution times, we will work on a partial dataset with only 5 categories out of the 20 available in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKg4Cey1oL_-"
      },
      "outputs": [],
      "source": [
        "%pip install ipytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoWcOgQ6oH5F"
      },
      "outputs": [],
      "source": [
        "categories = [\n",
        "    \"alt.atheism\",\n",
        "    \"soc.religion.christian\",\n",
        "    \"talk.religion.misc\",\n",
        "    \"comp.sys.ibm.pc.hardware\",\n",
        "    \"comp.sys.mac.hardware\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZqmauetoH5G"
      },
      "source": [
        "We load the documents from those categories, divided into train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVlX5_yroH5G"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "train = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=123)\n",
        "test = fetch_20newsgroups(subset=\"test\", categories=categories, shuffle=True, random_state=123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTluGAydoH5G"
      },
      "source": [
        "Check which categories got loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elONE3WioH5H"
      },
      "outputs": [],
      "source": [
        "print(train.target_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGOk1dWvoH5H"
      },
      "source": [
        "Check the size of training and test splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDujGY8-oH5I"
      },
      "outputs": [],
      "source": [
        "print(\"Training instances: {}\".format(len(train.data)))\n",
        "print(\"Test instances:     {}\".format(len(test.data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dq_tsoQoH5I"
      },
      "source": [
        "Check target labels of some of the train and test instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiABR6ceoH5I"
      },
      "outputs": [],
      "source": [
        "print(train.target[:10])\n",
        "print(test.target[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHJvC46loH5I"
      },
      "source": [
        "## Train a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McWGSX0ooH5I"
      },
      "source": [
        "Bag-of-words document representation, using raw term counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yRfdky8oH5J"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(train.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sye7ChTtoH5J"
      },
      "source": [
        "Check dimensionality (instances x features)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClGZnp0PoH5J"
      },
      "outputs": [],
      "source": [
        "print(X_train_counts.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GtRPAFVoH5J"
      },
      "source": [
        "Check vocabulary (sample 10 terms)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_IcAM83oH5J"
      },
      "outputs": [],
      "source": [
        "for idx, term in enumerate(count_vect.vocabulary_.keys()):\n",
        "    if idx < 10:\n",
        "        print(f\"{term} (ID: {count_vect.vocabulary_[term]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gT6aK_5oH5J"
      },
      "source": [
        "Learn a Naive Bayes model on the training data (by default it uses Laplace smoothing with alpha=1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8I2NjqhoH5K"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "classifier = MultinomialNB(alpha=1.0)\n",
        "classifier.fit(X_train_counts, train.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4fcYbqVoH5K"
      },
      "source": [
        "## Apply the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gLfZRX5oH5K"
      },
      "source": [
        "First, extract the same feature representation by re-using the `CountVectorizer` from before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AW2vhTVoH5K"
      },
      "outputs": [],
      "source": [
        "X_test_counts = count_vect.transform(test.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd53abgioH5K"
      },
      "source": [
        "Check dimensionality (documents x features)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lOprut1oH5K"
      },
      "outputs": [],
      "source": [
        "print(X_test_counts.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9W1STiWoH5K"
      },
      "source": [
        "Then, predict labels for test instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2GWNBQvoH5K"
      },
      "outputs": [],
      "source": [
        "predicted = classifier.predict(X_test_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OUTmj42oH5K"
      },
      "source": [
        "Look at some of the predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_shFX95oH5K"
      },
      "outputs": [],
      "source": [
        "print(predicted[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyBuR2SHoH5K"
      },
      "source": [
        "## Evaluate model performance\n",
        "\n",
        "We use Accuracy as our measure here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJxFjlyjoH5K"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(f\"{metrics.accuracy_score(test.target, predicted):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOSQAnZyoH5L"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "1) Use TF weighting instead of the raw counts. (See the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) for `TfidfTransformer` usage.)\n",
        "\n",
        "2) Try at least one different classifier, e.g., [linear SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) (or [other SVMs](https://scikit-learn.org/stable/modules/svm.html#svm-classification)).\n",
        "\n",
        "3) Record the results you got in the table below. How far can you push accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    switcher = {\n",
        "        'J': wordnet.ADJ,\n",
        "        'V': wordnet.VERB,\n",
        "        'N': wordnet.NOUN,\n",
        "        'R': wordnet.ADV\n",
        "    }\n",
        "    return switcher.get(treebank_tag[0], wordnet.NOUN)\n",
        "\n",
        "def tokenize_and_stem(text, use_stemmer: bool=False):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
        "    if use_stemmer:\n",
        "        stemmer = PorterStemmer()\n",
        "        stems = [stemmer.stem(item) for item in tokens]\n",
        "        return stems\n",
        "    else:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        wordnet_pos = [get_wordnet_pos(pos) for word, pos in pos_tag(tokens)]\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(token, pos) for token, pos in zip(tokens, wordnet_pos)]\n",
        "        return lemmatized_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_nb_raw = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_and_stem)),\n",
        "    ('clf', MultinomialNB()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_nb_tf = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_and_stem)),\n",
        "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
        "    ('clf', MultinomialNB()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_svm_raw = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_and_stem)),\n",
        "    ('clf', SGDClassifier()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_svm_tf = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_and_stem)),\n",
        "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
        "    ('clf', SGDClassifier()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for pipeline in [\n",
        "    pipeline_nb_raw, pipeline_nb_tf, \n",
        "    pipeline_svm_raw, pipeline_svm_tf\n",
        "]:\n",
        "    pipeline.fit(train.data, train.target)\n",
        "    predicted = pipeline.predict(test.data)\n",
        "    print(f\"{metrics.accuracy_score(test.target, predicted):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS674qV4oH5M"
      },
      "source": [
        "### Results\n",
        "\n",
        "| Model | Term weighting | Accuracy | Accuracy with Text Preprocessing (Stemming) | Accuracy with Text Preprocessing (Lemmatizing) |\n",
        "| -- | -- | -- | -- | -- |\n",
        "| Naive Bayes | Raw counts | 0.864 | 0.855 | 0.860 |\n",
        "| Naive Bayes | TF | 0.667 | 0.640 | 0.638 |\n",
        "| SVM | Raw counts | 0.822 | 0.764 | 0.798 |\n",
        "| SVM | TF | 0.852 | 0.830 | 0.822 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELOZQObgoH5M"
      },
      "source": [
        "## Optional exercise\n",
        "\n",
        "Can you push performance ever further? You could try, for example, more sophisticated text preprocessing (tokenization, stopwords removal, and stemming) using [NLTK](https://www.nltk.org/) (which is part of the Anaconda distribution). See, e.g., [this article](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a) for some hints."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
