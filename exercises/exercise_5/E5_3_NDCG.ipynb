{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qVOSf3HrEsJ"
      },
      "source": [
        "# NDCG Calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvAZgEjLrEsN"
      },
      "source": [
        "In this exercise, you'll have to evaluate system rankings, by computing the Normalized Discounted Cumulative Gain (NDCG) measure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhsQvECwrPAt"
      },
      "outputs": [],
      "source": [
        "%pip install ipytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reF4_yJRrEsN"
      },
      "outputs": [],
      "source": [
        "import ipytest\n",
        "import math\n",
        "import pytest\n",
        "\n",
        "from typing import Dict, List\n",
        "\n",
        "ipytest.autoconfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bUVtFGdrEsO"
      },
      "source": [
        "### Rankings produced for each query\n",
        "\n",
        "The key is the query ID (string), the value is a list of document IDs (ints)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7IqU1b7rEsO"
      },
      "outputs": [],
      "source": [
        "system_rankings = {\n",
        "    \"q1\": [2, 1, 3, 4, 5, 6, 10, 7, 9, 8],\n",
        "    \"q2\": [1, 2, 9, 4, 5, 6, 7, 8, 3, 10],\n",
        "    \"q3\": [1, 7, 4, 5, 3, 6, 9, 8, 10, 2]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g510QAGNrEsP"
      },
      "source": [
        "### Ground truth\n",
        "\n",
        "The key is the query ID, the value is a dictionary with (document ID, relevance) pairs. Relevance is measured on a 3-point scale: non-relevant (0), poor (1), good (2), excellent (3). Documents not listed here are non-relevant (relevance=0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRBH9-cwrEsP"
      },
      "outputs": [],
      "source": [
        "ground_truth = {\n",
        "    \"q1\": {4: 3, 1: 2, 2: 1},\n",
        "    \"q2\": {3: 3, 4: 3, 1: 2, 2: 1, 8: 1},\n",
        "    \"q3\": {1: 3, 4: 3, 7: 2, 5: 2, 6: 1, 8: 1}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97YrF5hMrEsQ"
      },
      "source": [
        "## Computing evaluation metrics\n",
        "\n",
        "Discounted cumulative gain at rank $k$ is computed as:\n",
        "\n",
        "$$DCG_k = rel_1 + \\sum_{i=2}^k\\frac{rel_i}{\\log_2 i}$$\n",
        "\n",
        "Normalized discounted cumulative gain at rank $k$ is computed as:\n",
        "\n",
        "$$NDCG_k = \\frac{DCG_k}{IDCG_k}$$\n",
        "\n",
        "where $IDCG_k$ is the $DCG_k$ score of an idealized (perfect) ranking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKX2Q9NQrEsQ"
      },
      "outputs": [],
      "source": [
        "def dcg(relevances: List[int], k: int) -> float:\n",
        "    \"\"\"Computes DCG@k, given the corresponding relevance levels for a ranked list of documents.\n",
        "\n",
        "    For example, given a ranking [2, 3, 1] where the relevance levels according to the ground\n",
        "    truth are {1:3, 2:4, 3:1}, the input list will be [4, 1, 3].\n",
        "\n",
        "    Args:\n",
        "        relevances: List with the ground truth relevance levels corresponding to a ranked list of documents.\n",
        "        k: Rank cut-off.\n",
        "\n",
        "    Returns:\n",
        "        DCG@k (float).\n",
        "    \"\"\"\n",
        "    dcg: float = relevances[0]\n",
        "    for i in range(1, min(k, len(relevances))):\n",
        "        dcg += relevances[i] / math.log2(i + 1)\n",
        "    return dcg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJlE1n80j4fq"
      },
      "source": [
        "Test DCG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YDeZ71Kigjj"
      },
      "outputs": [],
      "source": [
        "%%run_pytest[clean]\n",
        "\n",
        "@pytest.mark.parametrize(\"relevances,k,correct_value\", [\n",
        "    ([4, 1, 3], 2, 5.0),\n",
        "    ([4, 1, 3], 5, 6.893)\n",
        "])\n",
        "def test_dcg(relevances, k, correct_value):\n",
        "    assert dcg(relevances, k) == pytest.approx(correct_value, rel=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eR-iGFvrEsR"
      },
      "outputs": [],
      "source": [
        "def ndcg(system_ranking: List[int], ground_truth: Dict[int, int], k:int = 10) -> float:\n",
        "    \"\"\"Computes NDCG@k for a given system ranking.\n",
        "\n",
        "    Args:\n",
        "        system_ranking: Ranked list of document IDs (from most to least relevant).\n",
        "        ground_truth: Dict with document ID: relevance level pairs. Document not present here are to be taken with relevance = 0.\n",
        "        k: Rank cut-off.\n",
        "\n",
        "    Returns:\n",
        "        NDCG@k (float).\n",
        "    \"\"\"\n",
        "    # Holds corresponding relevance levels for the ranked docs.\n",
        "    relevances = [\n",
        "        ground_truth.get(doc_id, 0) for doc_id in system_ranking[:k]\n",
        "    ]\n",
        "    # Relevance levels of the idealized ranking.\n",
        "    relevances_ideal = [\n",
        "        ground_truth.get(doc_id, 0) for doc_id in sorted(ground_truth, key=ground_truth.get, reverse=True)[:k]\n",
        "    ]\n",
        "\n",
        "    return dcg(relevances, k) / dcg(relevances_ideal, k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-5M-ROrrEsS"
      },
      "source": [
        "Test NDCG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGGZN7qYrEsS"
      },
      "outputs": [],
      "source": [
        "%%run_pytest[clean]\n",
        "\n",
        "@pytest.mark.parametrize(\"qid,k,correct_value\", [\n",
        "    (\"q1\", 5, 0.799),\n",
        "    (\"q1\", 10, 0.799),\n",
        "    (\"q2\", 5, 0.549),\n",
        "    (\"q2\", 10, 0.705),\n",
        "    (\"q3\", 5, 0.908),\n",
        "    (\"q3\", 10, 0.949),\n",
        "])\n",
        "def test_queries(qid, k, correct_value):\n",
        "    assert ndcg(system_rankings[qid], ground_truth[qid], k) == pytest.approx(correct_value, rel=1e-3)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
